{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "fbf3d504",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "\n",
    "username = getpass.getuser()\n",
    "spark = SparkSession.builder \\\n",
    "    .config('spark.shuffle.useOldFetchProtocol','true') \\\n",
    "    .config(\"spark.ui.port\", \"0\") \\\n",
    "    .config('spark.sql.warehouse.dir', f'/user/{username}/warehouse') \\\n",
    "    .enableHiveSupport() \\\n",
    ".master('yarn') \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "cc6f75ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "8c81713f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e49e4ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# providing schema \n",
    "df_schema = \"user_id long, item_id string, rating double, category_id long\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "23b18734",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_food = spark.read \\\n",
    ".format('csv') \\\n",
    ".option('header',False) \\\n",
    ".schema(df_schema) \\\n",
    ".load('grocery/Grocery_and_Gourmet_Food.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1670f03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+------+-----------+\n",
      "|   user_id|       item_id|rating|category_id|\n",
      "+----------+--------------+------+-----------+\n",
      "|1888861614| ALP49FBWT4I7V|   5.0| 1370304000|\n",
      "|1888861614|A1KPIZOCLB9FZ8|   4.0| 1400803200|\n",
      "|1888861614|A2W0FA06IYAYQE|   4.0| 1399593600|\n",
      "|1888861614|A2PTZTCH2QUYBC|   5.0| 1397952000|\n",
      "|1888861614|A2VNHGJ59N4Z90|   4.0| 1397606400|\n",
      "|1888861614| ATQL0XOLZNHZ4|   1.0| 1392940800|\n",
      "|1888861614| A94ZG5CWN70E7|   5.0| 1385856000|\n",
      "|1888861614|A3QBT8YC3CZ7C9|   3.0| 1383696000|\n",
      "|1888861614| AGKW3A1RB1YGO|   5.0| 1380672000|\n",
      "|1888861614|A1B65IWLUVOUQB|   5.0| 1378425600|\n",
      "|1888861614|A34SURE2O1LJ4C|   5.0| 1377129600|\n",
      "|4639725183|A1QVBUH9E1V6I8|   5.0| 1416355200|\n",
      "|4639725183|A3F886P3E8L99T|   5.0| 1415577600|\n",
      "|4639725183|A2CHH5U12THP2D|   5.0| 1414972800|\n",
      "|4639725183|A29A6N9S9GDG6L|   5.0| 1409961600|\n",
      "|4639725183|A378HXZATS9HKM|   5.0| 1400630400|\n",
      "|4639725183|A2ZSQ8Y53ZNQK1|   5.0| 1395532800|\n",
      "|4639725183|A21ZU2TINEQE0H|   4.0| 1391817600|\n",
      "|4639725183| AFSBZNN8WDE8E|   5.0| 1389052800|\n",
      "|4639725183|A1XWZJZLZTMCWK|   5.0| 1388275200|\n",
      "+----------+--------------+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_food.show() # show some reocrds of the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca148538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5074160"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_food.count() # data contains arond 5 millions records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be4de2e",
   "metadata": {},
   "source": [
    "## Items having the least rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bccd75f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step one to find the average rating item wise\n",
    "# 2 step include sorting the data based on average rating\n",
    "items_with_least_rating = df_food.groupBy('item_id') \\\n",
    ".agg(round(mean('rating'),2).alias('average_rating')) \\\n",
    ".sort('average_rating',ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a793dab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+\n",
      "|       item_id|average_rating|\n",
      "+--------------+--------------+\n",
      "|A2LIHS415LATML|           1.0|\n",
      "|A17ST3YM5BIBER|           1.0|\n",
      "|A12E6LVLKQ3IOO|           1.0|\n",
      "|A3CEVCM0VXXCAU|           1.0|\n",
      "|A32O03IFN4D7LA|           1.0|\n",
      "|A2RPQKXC8BBNW9|           1.0|\n",
      "| AYJI1BHUGFMMA|           1.0|\n",
      "|A26XHAPDNQDGX4|           1.0|\n",
      "|A3VUKIUS48VVBE|           1.0|\n",
      "|A1UIQ8VAHWFKP9|           1.0|\n",
      "|A3130MWD7ATB3C|           1.0|\n",
      "|A2XXHSIYFF0RGJ|           1.0|\n",
      "| A6RV2Q3UARDEN|           1.0|\n",
      "|A3E0UM11TPJCUE|           1.0|\n",
      "|A3H0YLAEYYRGTA|           1.0|\n",
      "|A2LJQAAN4L814I|           1.0|\n",
      "|A2L43I5B9PPMHO|           1.0|\n",
      "|A1T007YW8HS2SP|           1.0|\n",
      "|A2I0V1PMQTSC4V|           1.0|\n",
      "|A3VL76XQGJEDTW|           1.0|\n",
      "+--------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "items_with_least_rating.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5db98e2",
   "metadata": {},
   "source": [
    "### Items having the most rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0ef8f79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_with_most_rating = df_food.groupBy('item_id') \\\n",
    ".agg(round(mean('rating'),2).alias('average_rating')) \\\n",
    ".sort('average_rating',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "93632a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+\n",
      "|       item_id|average_rating|\n",
      "+--------------+--------------+\n",
      "|A1ZWK7IW6H2FAP|           5.0|\n",
      "| ALDN272WIKK4O|           5.0|\n",
      "|A1F1L9NPKTS8VG|           5.0|\n",
      "|A220OR0RVHLYRW|           5.0|\n",
      "|A2ABAWEL5D5VKY|           5.0|\n",
      "|A3323BXQL4AX9N|           5.0|\n",
      "|A3FHEA1DI07RY9|           5.0|\n",
      "|A2ZZY9GHGWNA61|           5.0|\n",
      "|A1216PLO14YY8A|           5.0|\n",
      "|A2KVXV01UXW0QF|           5.0|\n",
      "|A3K4FHBFZDLRS8|           5.0|\n",
      "|A2YP6HFA6HZAKE|           5.0|\n",
      "| ACCY5W0Z2CNZB|           5.0|\n",
      "| AWSRBXQ639RA5|           5.0|\n",
      "|A12EIR6ZXRGSBL|           5.0|\n",
      "|A3OTF2L5DAQG3S|           5.0|\n",
      "| AB4BA1YLME0GE|           5.0|\n",
      "|A3PMPK39R9M0RP|           5.0|\n",
      "|A1PZ9BIG3R4O12|           5.0|\n",
      "|A199Z1JUVCGX64|           5.0|\n",
      "+--------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "items_with_most_rating .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02bc036",
   "metadata": {},
   "source": [
    "## Items with longest review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1225c4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DataType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3268dd3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------------------+\n",
      "|item_id|      date|              review|\n",
      "+-------+----------+--------------------+\n",
      "|    100|2001-12-01|             awesome|\n",
      "|    101|2003-10-13|Amazing product, ...|\n",
      "|    101|2003-11-19|   Not happy at all!|\n",
      "|    101|2003-11-13|The product is fa...|\n",
      "|    102|2003-11-23|Honestly, I expec...|\n",
      "|    103|2007-11-13|          Economical|\n",
      "|    101|2003-11-13|               worst|\n",
      "|    104|2003-01-10|I am absolutely d...|\n",
      "|    101|2003-11-13| I will but it again|\n",
      "|    105|2003-11-13|       Do not buy it|\n",
      "|    103|2004-11-01|I found it to be ...|\n",
      "|    106|2003-11-13|    It is exceptiona|\n",
      "|    100|2003-11-07|             Lovely!|\n",
      "|    102|2003-09-13|I regret buying i...|\n",
      "|    105|2003-12-13|     Pocket Friendly|\n",
      "+-------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# creating a data frame as the text review is not therer in the data set\n",
    "# below data contains three coluns, item id, date, and review\n",
    "data = [(100, \"2001-12-01\", 'awesome'),\n",
    "        (101, \"2003-10-13\", 'Amazing product, I am loving it'),\n",
    "          (101, \"2003-11-19\", 'Not happy at all!'),\n",
    "          (101, \"2003-11-13\", \"The product is fantastic! It's user-friendly, durable, and it has made my life so much easier.\"),\n",
    "          (102, \"2003-11-23\", \"Honestly, I expected more from this product. It didn't live up to the hype\"),\n",
    "          (103, \"2007-11-13\", 'Economical'),\n",
    "          (101, \"2003-11-13\", 'worst'),\n",
    "          (104, \"2003-01-10\", \"I am absolutely delighted with this product! The quality is outstanding, and it exceeded my expectations\"),\n",
    "          (101, \"2003-11-13\", 'I will but it again'),\n",
    "          (105, \"2003-11-13\", 'Do not buy it'),\n",
    "          (103, \"2004-11-01\", \"I found it to be quite frustrating to use. Disappointed\"),\n",
    "          (106, \"2003-11-13\", 'It is exceptiona'),\n",
    "          (100, \"2003-11-07\", 'Lovely!'),\n",
    "          (102, \"2003-09-13\", \"I regret buying it and would advise others to steer clear. Save your money for something better.\"),\n",
    "          (105, \"2003-12-13\", 'Pocket Friendly'),\n",
    "       ] # date is in year mm dd format\n",
    "\n",
    "# Define the schema\n",
    "custom_schema = StructType([\n",
    "    StructField(\"item_id\", StringType(), True),\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"review\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create a DataFrame\n",
    "df = spark.createDataFrame(data, schema=custom_schema)\n",
    "\n",
    "# Convert the \"date\" column to DateType using to_date function\n",
    "df = df.withColumn(\"date\", to_date(df[\"date\"], \"yyyy-MM-dd\"))\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "99fab355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|item_id|\n",
      "+-------+\n",
      "|    104|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@udf(IntegerType()) # function to find the lenght of reviews\n",
    "def review_lenght(string):\n",
    "    string = string.strip() # removing trailing spaces\n",
    "    return len(string)\n",
    "\n",
    "\n",
    "df =df.withColumn('review_length' , review_lenght('review'))\n",
    "\n",
    "\n",
    "\n",
    "filter_max_length = df.selectExpr('max(review_length) as review_length')\n",
    "filter_max_length = filter_max_length.first()[0]\n",
    "\n",
    "df_longest_review_item =df.filter(df['review_length']==filter_max_length).select('item_id')\n",
    "df_longest_review_item.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f38e76",
   "metadata": {},
   "source": [
    "### Transform: change the date MM-DD-YYYY format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "28ee6eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------------------+-------------+\n",
      "|item_id|      date|              review|review_length|\n",
      "+-------+----------+--------------------+-------------+\n",
      "|    100|12-01-2001|             awesome|            7|\n",
      "|    101|10-13-2003|Amazing product, ...|           31|\n",
      "|    101|11-19-2003|   Not happy at all!|           17|\n",
      "|    101|11-13-2003|The product is fa...|           94|\n",
      "|    102|11-23-2003|Honestly, I expec...|           74|\n",
      "|    103|11-13-2007|          Economical|           10|\n",
      "|    101|11-13-2003|               worst|            5|\n",
      "|    104|01-10-2003|I am absolutely d...|          104|\n",
      "|    101|11-13-2003| I will but it again|           19|\n",
      "|    105|11-13-2003|       Do not buy it|           13|\n",
      "|    103|11-01-2004|I found it to be ...|           55|\n",
      "|    106|11-13-2003|    It is exceptiona|           16|\n",
      "|    100|11-07-2003|             Lovely!|            7|\n",
      "|    102|09-13-2003|I regret buying i...|           96|\n",
      "|    105|12-13-2003|     Pocket Friendly|           15|\n",
      "+-------+----------+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df =df.withColumn('date' ,date_format('date','MM-dd-yyyy'))\n",
    "df.show()  # transformed date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b5ed61",
   "metadata": {},
   "source": [
    "###    Show a desired data frame operation which you learnt recently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2807b4",
   "metadata": {},
   "source": [
    "I learned recently joins , cache and broad cast join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d029a2f",
   "metadata": {},
   "source": [
    "### joins are mentioned below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "eece7876",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://g01.itversity.com:46573\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fe3e6750278>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21aa98f",
   "metadata": {},
   "source": [
    "# creating orders and customers table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "3f895efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orders Table:\n",
      "+-------+----------+----------+---------+\n",
      "|OrderID| OrderDate|CustomerID|   Status|\n",
      "+-------+----------+----------+---------+\n",
      "|      1|2022-01-01|      1001|  Pending|\n",
      "|      2|2022-01-02|      1002|  Shipped|\n",
      "|      3|2022-01-03|      1003|Delivered|\n",
      "|      4|2022-01-04|      1001|Delivered|\n",
      "|      5|2022-01-05|      1004|  Pending|\n",
      "|      6|2022-01-06|      1002|  Shipped|\n",
      "|      7|2022-01-07|      1005|  Pending|\n",
      "|      8|2022-01-08|      1001|  Shipped|\n",
      "|      9|2022-01-09|      1003|Delivered|\n",
      "|     10|2022-01-10|      1004|Delivered|\n",
      "+-------+----------+----------+---------+\n",
      "\n",
      "\n",
      "Customers Table:\n",
      "+----------+-------------+-------------------+\n",
      "|CustomerID| CustomerName|              Email|\n",
      "+----------+-------------+-------------------+\n",
      "|      1001|     John Doe|   john@example.com|\n",
      "|      1002|  Alice Smith|  alice@example.com|\n",
      "|      1003|  Bob Johnson|    bob@example.com|\n",
      "|      1004| Eva Williams|    eva@example.com|\n",
      "|      1005|Charlie Brown|charlie@example.com|\n",
      "+----------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a Spark session\n",
    "\n",
    "\n",
    "\n",
    "orders_data = [\n",
    "    (1, '2022-01-01', 1001, 'Pending'),\n",
    "    (2, '2022-01-02', 1002, 'Shipped'),\n",
    "    (3, '2022-01-03', 1003, 'Delivered'),\n",
    "    (4, '2022-01-04', 1001, 'Delivered'),\n",
    "    (5, '2022-01-05', 1004, 'Pending'),\n",
    "    (6, '2022-01-06', 1002, 'Shipped'),\n",
    "    (7, '2022-01-07', 1005, 'Pending'),\n",
    "    (8, '2022-01-08', 1001, 'Shipped'),\n",
    "    (9, '2022-01-09', 1003, 'Delivered'),\n",
    "    (10, '2022-01-10', 1004, 'Delivered'),\n",
    "]\n",
    "\n",
    "# Define schema for Orders table\n",
    "orders_schema = StructType([\n",
    "    StructField(\"OrderID\", IntegerType(), True),\n",
    "    StructField(\"OrderDate\", StringType(), True),\n",
    "    StructField(\"CustomerID\", IntegerType(), True),\n",
    "    StructField(\"Status\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create Orders DataFrame\n",
    "orders_df = spark.createDataFrame(orders_data, schema=orders_schema)\n",
    "\n",
    "\n",
    "customers_data = [\n",
    "    (1001, 'John Doe', 'john@example.com'),\n",
    "    (1002, 'Alice Smith', 'alice@example.com'),\n",
    "    (1003, 'Bob Johnson', 'bob@example.com'),\n",
    "    (1004, 'Eva Williams', 'eva@example.com'),\n",
    "    (1005, 'Charlie Brown', 'charlie@example.com'),\n",
    "]\n",
    "\n",
    "# Define schema for Customers table\n",
    "customers_schema = StructType([\n",
    "    StructField(\"CustomerID\", IntegerType(), True),\n",
    "    StructField(\"CustomerName\", StringType(), True),\n",
    "    StructField(\"Email\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create Customers DataFrame\n",
    "customers_df = spark.createDataFrame(customers_data, schema=customers_schema)\n",
    "\n",
    "# Show the created DataFrames\n",
    "print(\"Orders Table:\")\n",
    "orders_df.show()\n",
    "\n",
    "print(\"\\nCustomers Table:\")\n",
    "customers_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "f45b6266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inner join to show customers and numbers of orders that they ordered\n",
    "df_joined =customers_df.select('CustomerID','customerName') \\\n",
    ".join(orders_df.selectExpr('OrderID','CustomerID'), on= 'CustomerID',how ='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "fc206382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>CustomerID</th><th>customerName</th><th>OrderID</th></tr>\n",
       "<tr><td>1005</td><td>Charlie Brown</td><td>7</td></tr>\n",
       "<tr><td>1002</td><td>Alice Smith</td><td>2</td></tr>\n",
       "<tr><td>1002</td><td>Alice Smith</td><td>6</td></tr>\n",
       "<tr><td>1001</td><td>John Doe</td><td>1</td></tr>\n",
       "<tr><td>1001</td><td>John Doe</td><td>4</td></tr>\n",
       "<tr><td>1001</td><td>John Doe</td><td>8</td></tr>\n",
       "<tr><td>1003</td><td>Bob Johnson</td><td>3</td></tr>\n",
       "<tr><td>1003</td><td>Bob Johnson</td><td>9</td></tr>\n",
       "<tr><td>1004</td><td>Eva Williams</td><td>5</td></tr>\n",
       "<tr><td>1004</td><td>Eva Williams</td><td>10</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------+-------------+-------+\n",
       "|CustomerID| customerName|OrderID|\n",
       "+----------+-------------+-------+\n",
       "|      1005|Charlie Brown|      7|\n",
       "|      1002|  Alice Smith|      2|\n",
       "|      1002|  Alice Smith|      6|\n",
       "|      1001|     John Doe|      1|\n",
       "|      1001|     John Doe|      4|\n",
       "|      1001|     John Doe|      8|\n",
       "|      1003|  Bob Johnson|      9|\n",
       "|      1003|  Bob Johnson|      3|\n",
       "|      1004| Eva Williams|     10|\n",
       "|      1004| Eva Williams|      5|\n",
       "+----------+-------------+-------+"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "664f66d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>CustomerID</th><th>customerName</th><th>total_order</th></tr>\n",
       "<tr><td>1005</td><td>Charlie Brown</td><td>1</td></tr>\n",
       "<tr><td>1002</td><td>Alice Smith</td><td>2</td></tr>\n",
       "<tr><td>1001</td><td>John Doe</td><td>3</td></tr>\n",
       "<tr><td>1003</td><td>Bob Johnson</td><td>2</td></tr>\n",
       "<tr><td>1004</td><td>Eva Williams</td><td>2</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------+-------------+-----------+\n",
       "|CustomerID| customerName|total_order|\n",
       "+----------+-------------+-----------+\n",
       "|      1005|Charlie Brown|          1|\n",
       "|      1002|  Alice Smith|          2|\n",
       "|      1001|     John Doe|          3|\n",
       "|      1003|  Bob Johnson|          2|\n",
       "|      1004| Eva Williams|          2|\n",
       "+----------+-------------+-----------+"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_joined.groupBy('CustomerID','customerName').agg(count('OrderId').alias('total_order'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32f7d22",
   "metadata": {},
   "source": [
    "### cache "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "a8190c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>CustomerID</th><th>customerName</th><th>OrderID</th></tr>\n",
       "<tr><td>1005</td><td>Charlie Brown</td><td>7</td></tr>\n",
       "<tr><td>1002</td><td>Alice Smith</td><td>6</td></tr>\n",
       "<tr><td>1002</td><td>Alice Smith</td><td>2</td></tr>\n",
       "<tr><td>1001</td><td>John Doe</td><td>8</td></tr>\n",
       "<tr><td>1001</td><td>John Doe</td><td>1</td></tr>\n",
       "<tr><td>1001</td><td>John Doe</td><td>4</td></tr>\n",
       "<tr><td>1003</td><td>Bob Johnson</td><td>3</td></tr>\n",
       "<tr><td>1003</td><td>Bob Johnson</td><td>9</td></tr>\n",
       "<tr><td>1004</td><td>Eva Williams</td><td>5</td></tr>\n",
       "<tr><td>1004</td><td>Eva Williams</td><td>10</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "DataFrame[CustomerID: int, customerName: string, OrderID: int]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_joined.cache() # we have chached the data frame to the ram , now it can be read faster whenever it is required\n",
    "# while using cache we need to make sure it size should not be very high otherwise it would occupy the whole ram and increase execution time for\n",
    "# other task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "a36af8ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>CustomerID</th><th>customerName</th><th>OrderID</th></tr>\n",
       "<tr><td>1005</td><td>Charlie Brown</td><td>7</td></tr>\n",
       "<tr><td>1002</td><td>Alice Smith</td><td>6</td></tr>\n",
       "<tr><td>1002</td><td>Alice Smith</td><td>2</td></tr>\n",
       "<tr><td>1001</td><td>John Doe</td><td>8</td></tr>\n",
       "<tr><td>1001</td><td>John Doe</td><td>1</td></tr>\n",
       "<tr><td>1001</td><td>John Doe</td><td>4</td></tr>\n",
       "<tr><td>1003</td><td>Bob Johnson</td><td>3</td></tr>\n",
       "<tr><td>1003</td><td>Bob Johnson</td><td>9</td></tr>\n",
       "<tr><td>1004</td><td>Eva Williams</td><td>5</td></tr>\n",
       "<tr><td>1004</td><td>Eva Williams</td><td>10</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "DataFrame[CustomerID: int, customerName: string, OrderID: int]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_joined.unpersist() # this will remove the data frame from cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778ca155",
   "metadata": {},
   "source": [
    "# broad cast join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "1436c822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this join optimize the query when one table is large and another is very small\n",
    "# when this is apply, smaller table is copied to all the executor and join happend at local aggregation level, it avoids shuffling and save time\n",
    "# suppose customer table is smale, we can implemet broadcast join like\n",
    "\n",
    "# this join is applied automatically using Adaptive query optimization in latest version of spark that is beyond 3.2\n",
    "df_joined_broadcast =customers_df.select('CustomerID','customerName') \\\n",
    ".join(broadcast(orders_df.select('OrderID','CustomerID')), on= 'CustomerID',how ='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "accffce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------+\n",
      "|CustomerID| customerName|OrderID|\n",
      "+----------+-------------+-------+\n",
      "|      1001|     John Doe|      8|\n",
      "|      1001|     John Doe|      4|\n",
      "|      1001|     John Doe|      1|\n",
      "|      1002|  Alice Smith|      6|\n",
      "|      1002|  Alice Smith|      2|\n",
      "|      1003|  Bob Johnson|      9|\n",
      "|      1003|  Bob Johnson|      3|\n",
      "|      1004| Eva Williams|     10|\n",
      "|      1004| Eva Williams|      5|\n",
      "|      1005|Charlie Brown|      7|\n",
      "+----------+-------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_joined_broadcast.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b63f8b",
   "metadata": {},
   "source": [
    "# Saving file in parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "b6c454d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined_broadcast.write \\\n",
    ".format(\"parquet\") \\\n",
    ".mode('overwrite') \\\n",
    ".save(\"practiceOutput\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25b1467",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
